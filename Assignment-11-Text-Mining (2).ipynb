{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74740761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment-11-Text-Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c7fb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7935e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b558e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4910fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Text\n",
       "0           1                             @kunalb11 Im an alien\n",
       "1           2  @ID_AA_Carmack Ray tracing on Cyberpunk with H...\n",
       "2           3                @joerogan @Spotify Great interview!\n",
       "3           4                    @gtera27 Doge is underestimated\n",
       "4           5  @teslacn Congratulations Tesla China for amazi..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Admin\\Desktop\\Database\\11.Text Mining\\Elon_musk.csv',encoding=\"latin-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e4ee5",
   "metadata": {},
   "source": [
    "# Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48faa564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  word_count\n",
       "0                             @kunalb11 Im an alien           4\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...          13\n",
       "2                @joerogan @Spotify Great interview!           4\n",
       "3                    @gtera27 Doge is underestimated           4\n",
       "4  @teslacn Congratulations Tesla China for amazi...          17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Words in single tweet\n",
    "data['word_count'] = data['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['Text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f03284db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce9970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  char_count\n",
       "0                             @kunalb11 Im an alien          22\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...          82\n",
       "2                @joerogan @Spotify Great interview!          35\n",
       "3                    @gtera27 Doge is underestimated          31\n",
       "4  @teslacn Congratulations Tesla China for amazi...         104"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of characters in single tweet\n",
    "data['char_count'] = data['Text'].str.len() ## this also includes spaces\n",
    "data[['Text','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ae8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b017518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>5.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>5.176471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  avg_word\n",
       "0                             @kunalb11 Im an alien  4.750000\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...  5.384615\n",
       "2                @joerogan @Spotify Great interview!  8.000000\n",
       "3                    @gtera27 Doge is underestimated  7.000000\n",
       "4  @teslacn Congratulations Tesla China for amazi...  5.176471"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "data['avg_word'] = data['Text'].apply(lambda x: avg_word(x))\n",
    "data[['Text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b75acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40e57feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  stopwords\n",
       "0                             @kunalb11 Im an alien          1\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...          4\n",
       "2                @joerogan @Spotify Great interview!          0\n",
       "3                    @gtera27 Doge is underestimated          1\n",
       "4  @teslacn Congratulations Tesla China for amazi...          5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "data['stopwords'] = data['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "data[['Text','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e769c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19568f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  hastags\n",
       "0                             @kunalb11 Im an alien        1\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...        1\n",
       "2                @joerogan @Spotify Great interview!        2\n",
       "3                    @gtera27 Doge is underestimated        1\n",
       "4  @teslacn Congratulations Tesla China for amazi...        1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hastags'] = data['Text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
    "data[['Text','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "641a009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f044c0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  numerics\n",
       "0                             @kunalb11 Im an alien         0\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...         0\n",
       "2                @joerogan @Spotify Great interview!         0\n",
       "3                    @gtera27 Doge is underestimated         0\n",
       "4  @teslacn Congratulations Tesla China for amazi...         0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['numerics'] = data['Text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "data[['Text','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb5d2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Upper Case Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abccc4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@kunalb11 Im an alien</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ID_AA_Carmack Ray tracing on Cyberpunk with H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joerogan @Spotify Great interview!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gtera27 Doge is underestimated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@teslacn Congratulations Tesla China for amazi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  upper\n",
       "0                             @kunalb11 Im an alien      0\n",
       "1  @ID_AA_Carmack Ray tracing on Cyberpunk with H...      1\n",
       "2                @joerogan @Spotify Great interview!      0\n",
       "3                    @gtera27 Doge is underestimated      0\n",
       "4  @teslacn Congratulations Tesla China for amazi...      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['upper'] = data['Text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "data[['Text','upper']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70ffb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre - Processing\n",
    "# Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "114a27f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               @kunalb11 im an alien\n",
       "1    @id_aa_carmack ray tracing on cyberpunk with h...\n",
       "2                  @joerogan @spotify great interview!\n",
       "3                      @gtera27 doge is underestimated\n",
       "4    @teslacn congratulations tesla china for amazi...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c22a7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a5c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6624\\1947507549.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Text'] = data['Text'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                 kunalb11 im an alien\n",
       "1    id_aa_carmack ray tracing on cyberpunk with hd...\n",
       "2                     joerogan spotify great interview\n",
       "3                       gtera27 doge is underestimated\n",
       "4    teslacn congratulations tesla china for amazin...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'] = data['Text'].str.replace('[^\\w\\s]','')\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b299581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fafa089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    kunalb11 im alien\n",
       "1    id_aa_carmack ray tracing cyberpunk hdr nextle...\n",
       "2                     joerogan spotify great interview\n",
       "3                          gtera27 doge underestimated\n",
       "4    teslacn congratulations tesla china amazing ex...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04b6dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1c2611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacex            239\n",
       "amp               218\n",
       "tesla             166\n",
       "erdayastronaut    142\n",
       "rt                127\n",
       "ppathole          123\n",
       "flcnhvy           114\n",
       "yes                86\n",
       "great              76\n",
       "teslaownerssv      73\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e545131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    kunalb11 im alien\n",
       "1    id_aa_carmack ray tracing cyberpunk hdr nextle...\n",
       "2                           joerogan spotify interview\n",
       "3                          gtera27 doge underestimated\n",
       "4    teslacn congratulations china amazing executio...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db4d8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rare Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f823a5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nyquil                1\n",
       "musk                  1\n",
       "negati                1\n",
       "httpstco6ohta09s5l    1\n",
       "carousel              1\n",
       "joeingeneral          1\n",
       "andrewbogut           1\n",
       "typical               1\n",
       "unusual               1\n",
       "altho                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e66dee45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    kunalb11 im alien\n",
       "1    id_aa_carmack ray tracing cyberpunk hdr nextle...\n",
       "2                           joerogan spotify interview\n",
       "3                          gtera27 doge underestimated\n",
       "4    teslacn congratulations china amazing executio...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6e299f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a644bf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf2503ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6163d5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    kunalb11 in alien\n",
       "1    id_aa_carmack ray tracing cyberpunk her nextle...\n",
       "2                           joerogan specify interview\n",
       "3                          gtera27 done underestimated\n",
       "4    teslacn congratulations china amazing executio...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e5220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69a965b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['id_aa_carmack', 'ray', 'tracing', 'cyberpunk', 'hdr', 'nextlevel', 'tried'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(data['Text'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee2f0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39cf458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    kunalb11 im alien\n",
       "1    id_aa_carmack ray trace cyberpunk hdr nextleve...\n",
       "2                           joerogan spotifi interview\n",
       "3                              gtera27 doge underestim\n",
       "4    teslacn congratul china amaz execut last year ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "data['Text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6ca13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72e3e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b64b0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b5dd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77cd4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6c50509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "641c7de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['kunalb11', 'im']), WordList(['im', 'alien'])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(data['Text'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65c967c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_aa_carmack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ray</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tracing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cyberpunk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hdr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nextlevel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tried</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  tf\n",
       "0  id_aa_carmack   1\n",
       "1            ray   1\n",
       "2        tracing   1\n",
       "3      cyberpunk   1\n",
       "4            hdr   1\n",
       "5      nextlevel   1\n",
       "6          tried   1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (data['Text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55f0b4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_aa_carmack</td>\n",
       "      <td>1</td>\n",
       "      <td>4.166415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ray</td>\n",
       "      <td>1</td>\n",
       "      <td>5.035453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tracing</td>\n",
       "      <td>1</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cyberpunk</td>\n",
       "      <td>1</td>\n",
       "      <td>5.115496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hdr</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nextlevel</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tried</td>\n",
       "      <td>1</td>\n",
       "      <td>5.808643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  tf       idf\n",
       "0  id_aa_carmack   1  4.166415\n",
       "1            ray   1  5.035453\n",
       "2        tracing   1  7.600402\n",
       "3      cyberpunk   1  5.115496\n",
       "4            hdr   1  6.907255\n",
       "5      nextlevel   1  6.907255\n",
       "6          tried   1  5.808643"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(data.shape[0]/(len(data[data['Text'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b786797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency – Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "226f110c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_aa_carmack</td>\n",
       "      <td>1</td>\n",
       "      <td>4.166415</td>\n",
       "      <td>4.166415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ray</td>\n",
       "      <td>1</td>\n",
       "      <td>5.035453</td>\n",
       "      <td>5.035453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tracing</td>\n",
       "      <td>1</td>\n",
       "      <td>7.600402</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cyberpunk</td>\n",
       "      <td>1</td>\n",
       "      <td>5.115496</td>\n",
       "      <td>5.115496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hdr</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907255</td>\n",
       "      <td>6.907255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nextlevel</td>\n",
       "      <td>1</td>\n",
       "      <td>6.907255</td>\n",
       "      <td>6.907255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tried</td>\n",
       "      <td>1</td>\n",
       "      <td>5.808643</td>\n",
       "      <td>5.808643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  tf       idf     tfidf\n",
       "0  id_aa_carmack   1  4.166415  4.166415\n",
       "1            ray   1  5.035453  5.035453\n",
       "2        tracing   1  7.600402  7.600402\n",
       "3      cyberpunk   1  5.115496  5.115496\n",
       "4            hdr   1  6.907255  6.907255\n",
       "5      nextlevel   1  6.907255  6.907255\n",
       "6          tried   1  5.808643  5.808643"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc94ae39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1999x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7127 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "vect = tfidf.fit_transform(data['Text'])\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fb6f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "272bb1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1999x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7762 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "data_bow = bow.fit_transform(data['Text'])\n",
    "data_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04edb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a5c2fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 (-0.25, 0.75)\n",
       "1                                    (0.0, 0.0)\n",
       "2                                    (0.0, 0.0)\n",
       "3                                    (0.0, 0.0)\n",
       "4    (0.20000000000000004, 0.32222222222222224)\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2028d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kunalb11 im alien</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_aa_carmack ray tracing cyberpunk hdr nextle...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joerogan spotify interview</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gtera27 doge underestimated</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teslacn congratulations china amazing executio...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  sentiment\n",
       "0                                  kunalb11 im alien      -0.25\n",
       "1  id_aa_carmack ray tracing cyberpunk hdr nextle...       0.00\n",
       "2                         joerogan spotify interview       0.00\n",
       "3                        gtera27 doge underestimated       0.00\n",
       "4  teslacn congratulations china amazing executio...       0.20"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'] = data['Text'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "data[['Text','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72814d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (0.18.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b5abf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import matplotlib\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f65e772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e9a2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd803ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wordcloud in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (1.9.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.0)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (4.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install wordcloud\n",
    "! pip install tweepy\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "487a6e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World',\n",
       " ' Hello      World']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Preprocessing\n",
    "Text = \" Hello      World\"\n",
    "data =[Text.strip() for Text in Text1] \n",
    "# remove both the leading and the trailing characters\n",
    "data =[Text for Text1 in Text1 if Text] \n",
    "# removes empty strings, because they are considered in Python as False\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1fa91db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the list into one string/text\n",
    "data_text=' '.join(data)\n",
    "data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bdfa224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "# remove Twitter username handles from a given twitter text. (Removes @usernames)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "data_tokens=tknzr.tokenize(data_text)\n",
    "print(data_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0859d631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World Hello World'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again Joining the list into one string/text\n",
    "data_tokens_text=' '.join(data_tokens)\n",
    "data_tokens_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7473effe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World  Hello      World'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove https or url within text\n",
    "import re\n",
    "no_url_text=re.sub(r'http\\S+', '', data_text)\n",
    "no_url_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "880c17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World', 'Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text_tokens=word_tokenize(no_url_text)\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e508bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens count\n",
    "len(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cedb9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'W', 'r', 'l']\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "my_stop_words=stopwords.words('english')\n",
    "\n",
    "sw_list = ['\\x92','rt','ye','yeah','haha','Yes','U0001F923','I']\n",
    "my_stop_words.extend(sw_list)\n",
    "\n",
    "no_stop_tokens=[word for word in text_tokens if not word in my_stop_words]\n",
    "print(no_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e74bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f92a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming (Optional)\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "stemmed_tokens=[ps.stem(word) for word in lower_words]\n",
    "print(stemmed_tokens[100:200])\n",
    "for token in doc_block[100:200]:\n",
    "    print(token,token.pos_)   \n",
    "    # Normalize the data\n",
    "lower_words=[Text.lower() for Text in no_stop_tokens]\n",
    "print(lower_words[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install spacy\n",
    "lemmas=[token.lemma_ for token in doc]\n",
    "print(lemmas)\n",
    "clean_tweets=' '.join(lemmas)\n",
    "clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52feeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(r\"C:\\Users\\Admin\\Desktop\\Database\\11.Text Mining\\positive-words.txt\", \"r\", encoding=\"utf-8\") as p:\n",
    "    pos = p.read()\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(r\"C:\\Users\\Admin\\Desktop\\Database\\11.Text Mining\\negative-words.txt\", \"r\", encoding=\"ISO-8859-1\") as n:\n",
    "    neg = n.read()\n",
    "    print(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(40,30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS.add('pron')\n",
    "STOPWORDS.add('rt')\n",
    "STOPWORDS.add('yeah')\n",
    "wordcloud=WordCloud(width=3000,height=2000,background_color='black',max_words=50,\n",
    "                   colormap='Set1',stopwords=STOPWORDS).generate(clean_tweets)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cd520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(40,30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Generate Word Cloud\n",
    "\n",
    "STOPWORDS.add('pron')\n",
    "STOPWORDS.add('rt')\n",
    "STOPWORDS.add('yeah')\n",
    "wordcloud=WordCloud(width=3000,height=2000,background_color='black',max_words=50,\n",
    "                   colormap='Set1',stopwords=STOPWORDS).generate(clean_tweets)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(40,30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Generate Word Cloud\n",
    "\n",
    "STOPWORDS.add('pron')\n",
    "STOPWORDS.add('rt')\n",
    "STOPWORDS.add('yeah')\n",
    "wordcloud=WordCloud(width=3000,height=2000,background_color='black',max_words=50,\n",
    "                   colormap='Set1',stopwords=STOPWORDS).generate(clean_tweets)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c30bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
